{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from builtins import super\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: all the classes below have the input dimensions (and the input, come to think of it) hardwired into the code. Not nice, but oh well._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ML(object):\n",
    "    def __init__(self, name='ML'):\n",
    "        self.name = name\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('model'):\n",
    "                self.initialize_graph()\n",
    "            with tf.name_scope('global_ops'):\n",
    "                self.init = tf.initialize_all_variables()\n",
    "            \n",
    "    def initialize_graph(self):\n",
    "        \"\"\"Already in graph scope.\"\"\"\n",
    "        raise NotImplementedError('Method initialize_graph() must be implemented.')\n",
    "    \n",
    "    def inference(self, X):\n",
    "        \"\"\"Compute inference model over data X and return the result.\"\"\"\n",
    "        raise NotImplementedError('Method inference(X) must be implemented.')\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        \"\"\"Compute loss over training data X and expected outputs Y.\"\"\"\n",
    "        raise NotImplementedError('Method loss(X, Y) must be implemented.')\n",
    "    \n",
    "    def inputs(self):\n",
    "        \"\"\"Read/generate input training data X and expected outputs Y.\"\"\"\n",
    "        raise NotImplementedError('Method inputs() must be implemented.')\n",
    "    \n",
    "    def train(self, total_loss):\n",
    "        \"\"\"Train / adjust model parameters according to computed total loss.\"\"\"\n",
    "        raise NotImplementedError('Method train(total_loss) must be implemented.')\n",
    "        \n",
    "    def evaluate(self, sess, X, Y):\n",
    "        \"\"\"Evaluate the resulting trained model.\"\"\"\n",
    "        raise NotImplementedError('Method evaluate(sess, X, Y) must be implemented.')\n",
    "        \n",
    "    def run_training(self, training_steps, print_every=10, save_every=1000):\n",
    "        save_dir = os.path.join('saves', self.name)\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            X, Y = self.inputs()\n",
    "        \n",
    "            total_loss = self.loss(X, Y)\n",
    "            train_op = self.train(total_loss)\n",
    "            \n",
    "            # Create a saver.\n",
    "            saver = tf.train.Saver()\n",
    "                \n",
    "        sess = tf.Session(graph=self.graph)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        # Load the latest training checkpoint, if it exists\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            # Restores from checkpoint\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "        else:\n",
    "            sess.run(self.init)\n",
    "            initial_step = 0\n",
    "        \n",
    "        # The actual training loop\n",
    "        for step in range(initial_step, training_steps):\n",
    "            sess.run([train_op])\n",
    "            \n",
    "            # for debugging and learning purposes, see how the loss gets\n",
    "            # decremented thru training steps\n",
    "            if step % print_every == 0:\n",
    "                print('loss: ', sess.run(total_loss))\n",
    "            \n",
    "            # Model checkpoint\n",
    "            if step % save_every == 0 and step > 0:\n",
    "                print('Saving in...')\n",
    "                saver.save(sess, save_dir, global_step=step)\n",
    "                \n",
    "        print('Saving out...')\n",
    "        saver.save(sess, save_dir, global_step=training_steps)\n",
    "        \n",
    "        self.evaluate(sess, X, Y)\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "     \n",
    "    @staticmethod\n",
    "    def read_csv(batch_size, file_name, record_defaults):\n",
    "        filename_queue = tf.train.string_input_producer([file_name])\n",
    "        reader = tf.TextLineReader(skip_header_lines=1)\n",
    "        key, value = reader.read(filename_queue)\n",
    "        \n",
    "        # decode_csv will convert a Tensor from type string (the text line) in\n",
    "        # a tuple of tensor columns with the specified defaults, which also\n",
    "        # sets the data type for each column\n",
    "        decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "        \n",
    "        # batch actually reads the file and loads \"batch_size\" rows in a single tensor\n",
    "        return tf.train.shuffle_batch(decoded,\n",
    "                                      batch_size=batch_size,\n",
    "                                      capacity=batch_size * 50,\n",
    "                                      min_after_dequeue=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(ML):\n",
    "    def __init__(self, name='LR'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def initialize_graph(self):\n",
    "        # initialize variables/model parameters\n",
    "        self.W = tf.Variable(tf.zeros([2, 1]), name=\"weights\")\n",
    "        self.b = tf.Variable(0., name=\"bias\")\n",
    "        \n",
    "    def inference(self, X):\n",
    "        return tf.matmul(X, self.W) + self.b\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        beta = 0.001\n",
    "        Y_predicted = self.inference(X)\n",
    "        return (tf.reduce_sum(tf.squared_difference(Y, Y_predicted))) #+\n",
    "                #beta * (tf.nn.l2_loss(self.W) + tf.nn.l2_loss(self.b)))\n",
    "    \n",
    "    def inputs(self):\n",
    "        weight_age = [[84, 46], [73, 20], [65, 52], [70, 30], [76, 57],\n",
    "                      [69, 25], [63, 28], [72, 36], [79, 57], [75, 44],\n",
    "                      [27, 24], [89, 31], [65, 52], [57, 23], [59, 60],\n",
    "                      [69, 48], [60, 34], [79, 51], [75, 50], [82, 34],\n",
    "                      [59, 46], [67, 23], [85, 37], [55, 40], [63, 30]]\n",
    "        blood_fat_content = [354, 190, 405, 263, 451,\n",
    "                             302, 288, 385, 402, 365,\n",
    "                             209, 290, 346, 254, 395,\n",
    "                             434, 220, 374, 308, 220,\n",
    "                             311, 181, 274, 303, 244]\n",
    "        return tf.to_float(weight_age), tf.to_float(blood_fat_content)\n",
    "\n",
    "    def train(self, total_loss):\n",
    "        learning_rate = 0.0000001\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "        \n",
    "    \n",
    "    def evaluate(self, sess, X, Y):\n",
    "        print(sess.run(self.inference([[80., 23.]])))\n",
    "        print(sess.run(self.inference([[65., 23.]])))\n",
    "        print(sess.run(self.inference([[57., 23.]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  7.60877e+06\n",
      "loss:  5.22567e+06\n",
      "loss:  5.11695e+06\n",
      "loss:  5.01565e+06\n",
      "loss:  4.92128e+06\n",
      "loss:  4.83335e+06\n",
      "loss:  4.75144e+06\n",
      "loss:  4.67511e+06\n",
      "loss:  4.60401e+06\n",
      "loss:  4.53777e+06\n",
      "Saving out...\n",
      "[[ 315.59927368]]\n",
      "[[ 278.35565186]]\n",
      "[[ 258.49240112]]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.run_training(100000, print_every=10000, save_every=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(ML):\n",
    "    def __init__(self, name='LR'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def initialize_graph(self):\n",
    "        # initialize variables/model parameters\n",
    "        self.W = tf.Variable(tf.zeros([5, 1]), name=\"weights\")\n",
    "        self.b = tf.Variable(0., name=\"bias\")\n",
    "        \n",
    "    def combine_inputs(self, X):\n",
    "        return tf.matmul(X, self.W) + self.b\n",
    "        \n",
    "    def inference(self, X):\n",
    "        return tf.sigmoid(self.combine_inputs(X))\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        beta = 0.001\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.combine_inputs(X), Y))\n",
    "    \n",
    "    def inputs(self):\n",
    "        (passenger_id, survived, pclass, name, sex, age,\n",
    "         sibsp, parch, ticket, fare, cabin, embarked) = self.read_csv(\n",
    "            100, 'data/titanic_train.csv',\n",
    "            [[0.0], [0.0], [0], [\"\"], [\"\"], [0.0],\n",
    "             [0.0], [0.0], [\"\"], [0.0], [\"\"], [\"\"]])\n",
    "        \n",
    "        # convert categorical data\n",
    "        is_first_class = tf.to_float(tf.equal(pclass, [1]))\n",
    "        is_second_class = tf.to_float(tf.equal(pclass, [2]))\n",
    "        is_third_class = tf.to_float(tf.equal(pclass, [3]))\n",
    "        gender = tf.to_float(tf.equal(sex, [\"female\"]))\n",
    "        \n",
    "        # Finally we pack all the features in a single matrix;\n",
    "        # We then transpose to have a matrix with one example per row and one\n",
    "        # feature per column.\n",
    "        features = tf.transpose(tf.pack([is_first_class, is_second_class,\n",
    "                                         is_third_class, gender, age]))\n",
    "        survived = tf.reshape(survived, [100, 1])\n",
    "        return features, survived\n",
    "\n",
    "    def train(self, total_loss):\n",
    "        learning_rate = 0.01\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "        \n",
    "    def evaluate(self, sess, X, Y):\n",
    "        # Accuracy: how many correct predictions on average?\n",
    "        # We have to convert the bool values to to float (T/F -> 1/0)\n",
    "        with self.graph.as_default():\n",
    "            predicted = tf.cast(self.inference(X) > 0.5, tf.float32)\n",
    "            print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.625054\n",
      "loss:  0.587274\n",
      "loss:  0.454687\n",
      "loss:  0.46095\n",
      "loss:  0.406245\n",
      "loss:  0.518605\n",
      "Saving in...\n",
      "loss:  0.423056\n",
      "loss:  0.508384\n",
      "loss:  0.415439\n",
      "loss:  0.414944\n",
      "Saving out...\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "lore = LogisticRegression()\n",
    "lore.run_training(20000, print_every=2000, save_every=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression(ML):\n",
    "    def __init__(self, name='LR'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def initialize_graph(self):\n",
    "        # initialize variables/model parameters\n",
    "        self.W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n",
    "        self.b = tf.Variable(tf.zeros([3]), name=\"bias\")\n",
    "        \n",
    "    def combine_inputs(self, X):\n",
    "        return tf.matmul(tf.cast(X, tf.float32), self.W) + self.b\n",
    "        \n",
    "    def inference(self, X):\n",
    "        return tf.nn.softmax(self.combine_inputs(X))\n",
    "    \n",
    "    def loss(self, X, Y):\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            self.combine_inputs(X), Y))\n",
    "    \n",
    "    def inputs(self):\n",
    "        sepal_length, sepal_width, petal_length, petal_width, label =\\\n",
    "            self.read_csv(50, \"./data/iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "\n",
    "        # Convert class names to a 0 based class index.\n",
    "        # The last to_int32 is because argmax returns int64\n",
    "        label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.pack([\n",
    "            tf.equal(label, [\"Iris-setosa\"]),\n",
    "            tf.equal(label, [\"Iris-versicolor\"]),\n",
    "            tf.equal(label, [\"Iris-virginica\"])])), 0))\n",
    "        \n",
    "        # Pack all the features that we care about in a single matrix;\n",
    "        # We then transpose to have a matrix with one example per row and one\n",
    "        # feature per column.\n",
    "        features = tf.transpose(tf.pack([sepal_length, sepal_width, petal_length, petal_width]))\n",
    "        return features, label_number\n",
    "        \n",
    "    def train(self, total_loss):\n",
    "        learning_rate = 0.01\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "        \n",
    "    def evaluate(self, sess, X, Y):\n",
    "        # Accuracy: how many correct predictions on average?\n",
    "        # We have to convert the bool values to to float (T/F -> 1/0)\n",
    "        with self.graph.as_default():\n",
    "            predicted = tf.cast(tf.argmax(self.inference(X), 1), tf.int32)\n",
    "            print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.14878\n",
      "loss:  0.374805\n",
      "loss:  0.32957\n",
      "loss:  0.21796\n",
      "loss:  0.20724\n",
      "loss:  0.128124\n",
      "loss:  0.139673\n",
      "loss:  0.125353\n",
      "loss:  0.163467\n",
      "loss:  0.175137\n",
      "loss:  0.168669\n",
      "Saving in...\n",
      "loss:  0.154647\n",
      "loss:  0.144735\n",
      "loss:  0.142105\n",
      "loss:  0.128798\n",
      "Saving out...\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "mlr = MultinomialLogisticRegression()\n",
    "mlr.run_training(15000, print_every=1000, save_every=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
